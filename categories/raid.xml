<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0"><channel><title>Adventurous Computing (Posts about raid)</title><link>https://blog.cyplo.net/</link><description></description><atom:link rel="self" type="application/rss+xml" href="https://blog.cyplo.net/categories/raid.xml"></atom:link><language>en</language><lastBuildDate>Tue, 01 Jan 2019 12:49:13 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Building NAS - software</title><link>https://blog.cyplo.net/posts/2013/05/26/building-nas-software.html</link><dc:creator>Cyryl Płotnicki</dc:creator><description>&lt;div&gt;&lt;div class="section" id="operating-system"&gt;
&lt;h2&gt;Operating system&lt;/h2&gt;
&lt;p&gt;I think my NAS box build is no longer in much flux, so I thought it'd be
nice to describe it. I had some disks laying around, I had them
installed and started playing with the software setup.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_fe22866c275545db84f051dfcdefb70e-1"&gt;&lt;/a&gt;Disk /dev/sda:  60.0 GB,  60022480896 bytes
&lt;a name="rest_code_fe22866c275545db84f051dfcdefb70e-2"&gt;&lt;/a&gt;Disk /dev/sdb: 320.1 GB, 320072933376 bytes
&lt;a name="rest_code_fe22866c275545db84f051dfcdefb70e-3"&gt;&lt;/a&gt;Disk /dev/sdc: 160.0 GB, 160041885696 bytes
&lt;a name="rest_code_fe22866c275545db84f051dfcdefb70e-4"&gt;&lt;/a&gt;Disk /dev/sdd: 250.1 GB, 250059350016 bytes
&lt;a name="rest_code_fe22866c275545db84f051dfcdefb70e-5"&gt;&lt;/a&gt;Disk /dev/sde: 500.1 GB, 500107862016 bytes
&lt;/pre&gt;&lt;p&gt;First one is an SSD drive, I used it for OS
installation.  I went for &lt;a class="reference external" href="http://crunchbang.org/"&gt;Crunchbang&lt;/a&gt; as I
was already familiar with it, however now I'm thinking of just getting
newest Debian there, as it's finally released. Nothing fancy about the
OS, a regular install really.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_6496e8db00c74276b61932ad13b2cdae-1"&gt;&lt;/a&gt;storage# df -h
&lt;a name="rest_code_6496e8db00c74276b61932ad13b2cdae-2"&gt;&lt;/a&gt;Filesystem                   Size  Used Avail Use% Mounted on
&lt;a name="rest_code_6496e8db00c74276b61932ad13b2cdae-3"&gt;&lt;/a&gt;rootfs                        53G  2.4G   48G   5% /
&lt;a name="rest_code_6496e8db00c74276b61932ad13b2cdae-4"&gt;&lt;/a&gt;/dev/sda1                    461M   31M  407M   7% /boot
&lt;/pre&gt;&lt;p&gt;As you can see / filesystem takes little amount
of space, hence the next thing I plan on doing is actually move / to
USB3.0 pendrive and then free the SATA drive from it's current duties.
I'm reluctant to do so right now, as moving swap to pendrive might
result in significant wear. I'm thinking of getting more RAM and then
getting rid of the swap at all. These stats were acquired after reboot,
there are some loads under which I saw swapping occur.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_e6769a76f269406d862b4e31cbb77e1c-1"&gt;&lt;/a&gt;storage# free -m
&lt;a name="rest_code_e6769a76f269406d862b4e31cbb77e1c-2"&gt;&lt;/a&gt;             total       used       free     shared    buffers
&lt;a name="rest_code_e6769a76f269406d862b4e31cbb77e1c-3"&gt;&lt;/a&gt;Mem:          1636        282       1354          0         53
&lt;a name="rest_code_e6769a76f269406d862b4e31cbb77e1c-4"&gt;&lt;/a&gt;-/+ buffers/cache:        166       1470
&lt;a name="rest_code_e6769a76f269406d862b4e31cbb77e1c-5"&gt;&lt;/a&gt;Swap:         1903          0       1903
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="software-configuration"&gt;
&lt;h2&gt;Software + configuration&lt;/h2&gt;
&lt;p&gt;NAS means SAMBA, right ? That's what I though. RAID5 + SAMBA for Win
clients and NFS for others. After a while I got accustomed to this setup
and started playing with my photo collection as it was laying on NAS.
The problem ? I deleted one photo and wanted it back. It was nowhere to
be found. RAID5, although having internal copies for resiliency, was
visible as one drive only and happily deleted the data when asked to.
What I really needed was a backup solution, not a NAS. My final answer
to that:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_6b37e4d1165e40e9ab756c3d65b67c14-1"&gt;&lt;/a&gt;storage# df -h
&lt;a name="rest_code_6b37e4d1165e40e9ab756c3d65b67c14-2"&gt;&lt;/a&gt;Filesystem                   Size  Used Avail Use% Mounted on
&lt;a name="rest_code_6b37e4d1165e40e9ab756c3d65b67c14-3"&gt;&lt;/a&gt;rootfs                        53G  2.4G   48G   5% /
&lt;a name="rest_code_6b37e4d1165e40e9ab756c3d65b67c14-4"&gt;&lt;/a&gt;/dev/md0                     294G   36G  243G  13% /mnt/array_back
&lt;a name="rest_code_6b37e4d1165e40e9ab756c3d65b67c14-5"&gt;&lt;/a&gt;/dev/sde1                    459G   35G  401G   8% /mnt/array_front
&lt;a name="rest_code_6b37e4d1165e40e9ab756c3d65b67c14-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_6b37e4d1165e40e9ab756c3d65b67c14-7"&gt;&lt;/a&gt;storage# cat /etc/fstab
&lt;a name="rest_code_6b37e4d1165e40e9ab756c3d65b67c14-8"&gt;&lt;/a&gt;#
&lt;a name="rest_code_6b37e4d1165e40e9ab756c3d65b67c14-9"&gt;&lt;/a&gt;/dev/mapper/vg_storage-root                /               ext4    errors=remount-ro 0       1
&lt;a name="rest_code_6b37e4d1165e40e9ab756c3d65b67c14-10"&gt;&lt;/a&gt;UUID=b9d32208-edc0-4981-ab74-5da1e7348a1a  /boot           ext4    defaults          0       2
&lt;a name="rest_code_6b37e4d1165e40e9ab756c3d65b67c14-11"&gt;&lt;/a&gt;/dev/mapper/vg_storage-swap                none            swap    sw                0       0
&lt;a name="rest_code_6b37e4d1165e40e9ab756c3d65b67c14-12"&gt;&lt;/a&gt;
&lt;a name="rest_code_6b37e4d1165e40e9ab756c3d65b67c14-13"&gt;&lt;/a&gt;/dev/md0                                  /mnt/array_back  ext4    defaults          0       2
&lt;a name="rest_code_6b37e4d1165e40e9ab756c3d65b67c14-14"&gt;&lt;/a&gt;/dev/sde1                                 /mnt/array_front ext4    defaults          0       2
&lt;a name="rest_code_6b37e4d1165e40e9ab756c3d65b67c14-15"&gt;&lt;/a&gt;
&lt;a name="rest_code_6b37e4d1165e40e9ab756c3d65b67c14-16"&gt;&lt;/a&gt;storage# mdadm --detail /dev/md0
&lt;a name="rest_code_6b37e4d1165e40e9ab756c3d65b67c14-17"&gt;&lt;/a&gt;/dev/md0:
&lt;a name="rest_code_6b37e4d1165e40e9ab756c3d65b67c14-18"&gt;&lt;/a&gt;        Version : 1.2
&lt;a name="rest_code_6b37e4d1165e40e9ab756c3d65b67c14-19"&gt;&lt;/a&gt;  Creation Time : Sun Apr 21 22:47:38 2013
&lt;a name="rest_code_6b37e4d1165e40e9ab756c3d65b67c14-20"&gt;&lt;/a&gt;     Raid Level : raid5
&lt;a name="rest_code_6b37e4d1165e40e9ab756c3d65b67c14-21"&gt;&lt;/a&gt;     Array Size : 312318976 (297.85 GiB 319.81 GB)
&lt;a name="rest_code_6b37e4d1165e40e9ab756c3d65b67c14-22"&gt;&lt;/a&gt;  Used Dev Size : 156159488 (148.93 GiB 159.91 GB)
&lt;a name="rest_code_6b37e4d1165e40e9ab756c3d65b67c14-23"&gt;&lt;/a&gt;
&lt;a name="rest_code_6b37e4d1165e40e9ab756c3d65b67c14-24"&gt;&lt;/a&gt;    Number   Major   Minor   RaidDevice State
&lt;a name="rest_code_6b37e4d1165e40e9ab756c3d65b67c14-25"&gt;&lt;/a&gt;       0       8       17        0      active sync   /dev/sdb1
&lt;a name="rest_code_6b37e4d1165e40e9ab756c3d65b67c14-26"&gt;&lt;/a&gt;       1       8       33        1      active sync   /dev/sdc1
&lt;a name="rest_code_6b37e4d1165e40e9ab756c3d65b67c14-27"&gt;&lt;/a&gt;       3       8       49        2      active sync   /dev/sdd1
&lt;/pre&gt;&lt;p&gt;One disk [sde] serves as a front for all user operations. After a while, all changes
except for deletions are being flushed onto [array_back] which is a
RAID5 matrix.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_6fe335a50da14cde9ec86a5770916a30-1"&gt;&lt;/a&gt;storage# cat /etc/cron.daily/90_sync_front_to_back
&lt;a name="rest_code_6fe335a50da14cde9ec86a5770916a30-2"&gt;&lt;/a&gt;#!/bin/bash
&lt;a name="rest_code_6fe335a50da14cde9ec86a5770916a30-3"&gt;&lt;/a&gt;rsync -avr /mnt/array_front/ /mnt/array_back/back
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="secret-sauce"&gt;
&lt;h2&gt;Secret sauce&lt;/h2&gt;
&lt;p&gt;&lt;a class="reference external" href="http://owncloud.org/"&gt;ownCloud&lt;/a&gt;. [array_front] is not directly
exposed via SAMBA or NFS, it's governed by ownCloud instance, and then
only ownCloud sync client on the computer or phone gets to mess with the
data. By having such setup I get 3 copies of each file. One on device,
one on the front array and one on the back array. What is also cool
about ownCloud is that it also handles contacts and calendar storage for
me. One more step towards getting all my data off google ! Points for
improvement:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;[array_front] is not an array now. It's just a disk. Make it an
proper disk array.&lt;/li&gt;
&lt;li&gt;encrypt the data from array_back and send it to S3 and then let it
graduate to Glacier&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>crunchbang</category><category>debian</category><category>nas</category><category>owncloud</category><category>raid</category><guid>https://blog.cyplo.net/posts/2013/05/26/building-nas-software.html</guid><pubDate>Sun, 26 May 2013 21:32:07 GMT</pubDate></item></channel></rss>